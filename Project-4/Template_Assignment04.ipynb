{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Iqxc0E5rFc_"
      },
      "source": [
        "# This is the python template for Assignment 04.  \n",
        "- You must use this template.  \n",
        "- You must not change any signatures of the methods, only edit the sections indicated with \"Write your code here.\"  \n",
        "- The return of every function has to be in the right format, otherwise this is a desk reject.  \n",
        "- Plagiarism leads to failing the assignment!  \n",
        "- We will terminate the script after 10 min, try to use efficient algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VFHU4BCNrFdC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C72-HOUjrFdF"
      },
      "outputs": [],
      "source": [
        "def get_name():\n",
        "    return \"Irem Begüm Gündüz\"\n",
        "def get_matriculationnumber():\n",
        "    return 7026821"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IizOf4UzrFdI"
      },
      "source": [
        "## Useful information:\n",
        "\n",
        "The structure of a CART is a dict. Use the same names as shown in the example, using other names makes your format invalid and leads to a desk reject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L_e1560zrFdJ"
      },
      "outputs": [],
      "source": [
        "cart = { \"name\":\"X\", \"mean\":456, \"split_by_feature\": \"aes\", \"error_of_split\": 0.0,\n",
        "        \"successor_left\": { \"name\":\"XL\", \"mean\":1234, \"split_by_feature\": None, \"error_of_split\":None,\n",
        "                           \"successor_left\":None,\n",
        "                           \"successor_right\":None\n",
        "                          },\n",
        "        \"successor_right\":{ \"name\":\"XR\", \"mean\":258, \"split_by_feature\": None,\"error_of_split\":None,\n",
        "                           \"successor_left\":None,\n",
        "                           \"successor_right\":None\n",
        "                          }\n",
        "       }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwvVaIz9rFdL"
      },
      "source": [
        "The names of the features must be used as defined in this list, using other names makes your format invalid and leads to a desk reject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "68cwhzvCrFdL"
      },
      "outputs": [],
      "source": [
        "features = [\"secompress\", \"encryption\", \"aes\", \"blowfish\", \"algorithm\", \"rar\", \"zip\", \"signature\",\n",
        "            \"timestamp\", \"segmentation\", \"onehundredmb\", \"onegb\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BF_1CHLrFdL"
      },
      "source": [
        "# Task 1: Create a CART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wICjZ9XmrFdM"
      },
      "outputs": [],
      "source": [
        "def compute_performance(dataframe,dataframe2=pd.DataFrame(),metric= \"performance\"):\n",
        "  #compute the performance of given dataframe(S)\n",
        "    if  dataframe2.empty:\n",
        "         feature = extract_features(dataframe)\n",
        "         results_table = compute_performance_helper(dataframe,feature, metric)\n",
        "         return results_table\n",
        "    else:\n",
        "       features,features2 = extract_features(dataframe,dataframe2)\n",
        "       results_table = compute_performance_helper(dataframe,features, metric)\n",
        "       results_table2 = compute_performance_helper(dataframe2,features2, metric)\n",
        "       return results_table, results_table2\n",
        "\n",
        "def compute_performance_helper(dataframe,features, metric= \"performance\"):\n",
        "    # Initialize a results dataframe\n",
        "    results_table = results_helper_df()\n",
        "    \n",
        "    for feature in features:\n",
        "        # Split the dataframe into two based on the feature\n",
        "        left_df = dataframe[dataframe[feature] == 1]\n",
        "        right_df  = dataframe[dataframe[feature] == 0]\n",
        "\n",
        "        # Sum the values \n",
        "        left_sum = left_df[metric].sum()\n",
        "        right_sum = right_df [metric].sum()\n",
        "\n",
        "        # Calculate the mean of the performance \n",
        "        if len(left_df) != 0: \n",
        "          left_mean = round(left_sum / len(left_df),2) \n",
        "        else: \n",
        "          left_mean = 0\n",
        "\n",
        "        if len(right_df ) != 0: \n",
        "          right_mean = round(right_sum / len(right_df ),2)\n",
        "        else: \n",
        "          right_mean = 0\n",
        "\n",
        "        # Calculate the squared error \n",
        "        left_sq_err = round(((left_df[metric] - left_mean) ** 2).sum(), 2)\n",
        "        right_sq_err = round(((right_df [metric] - right_mean) ** 2).sum(), 2)\n",
        "\n",
        "        # return the total squared error per feature\n",
        "        total_error = left_sq_err + right_sq_err\n",
        "        result = [feature, total_error]\n",
        "        results_table.loc[len(results_table)] = result\n",
        "    return results_table \n",
        "\n",
        "def results_helper_df():\n",
        "    #create an empty dataframe\n",
        "    results_df = pd.DataFrame(columns=['feature', 'total_error'])\n",
        "    return results_df\n",
        "\n",
        "def extract_features(df,df2=pd.DataFrame()):\n",
        "    #extract features from columns by disgarding id and performance\n",
        "    #make the features lower so, we fit the criteria for feature names\n",
        "    features = [col.lower() for col in df.columns if col != 'performance' and col != 'Id']\n",
        "    if  df2.size == 0: \n",
        "      return features\n",
        "    else :\n",
        "      features2 = [col.lower() for col in df2.columns if col != 'performance' and col != 'Id']\n",
        "      return features,features2\n",
        "\n",
        "def get_split_var(df, tot_error_col='total_error', feature_col='feature'):\n",
        "  # sanity checking\n",
        "  if df[tot_error_col].isna().any() or df[tot_error_col].min() == None:\n",
        "    return None\n",
        "  # check if minimum value in the tot_error_col is None\n",
        "  min_error = df[tot_error_col].min()\n",
        "\n",
        "  # Check if more than one min exists\n",
        "  counts_min = df[tot_error_col].value_counts()[min_error]\n",
        "\n",
        "  # if there is only one feature with the min error extract the split var\n",
        "  if counts_min == 1:\n",
        "    split_var = df.loc[df[tot_error_col] == min_error, feature_col].values[0]\n",
        "  else:\n",
        "    df_min = df.query(f'{tot_error_col} == @min_error')\n",
        "    #if all features have the same min error return none\n",
        "    if df_min[feature_col].nunique() == df[feature_col].nunique():\n",
        "       return None\n",
        "     #else sort the features alphabetically, return the first one\n",
        "    split_var = df_min[feature_col].sort_values().values[0]\n",
        "  return split_var\n",
        "\n",
        "\n",
        "def build_tree(df, results=pd.DataFrame(), feature_col='feature', totError='total_error', name='X'):\n",
        "  if results.empty:\n",
        "    #compute the initial performance \n",
        "    results = compute_performance(df, metric = 'performance')\n",
        "    \n",
        "  #start an empty tree \n",
        "  tree = {}\n",
        "  tree['name'] = name\n",
        "  tree['mean'] = df['performance'].mean()\n",
        "  splitting_var = get_split_var(results)\n",
        "    \n",
        "  # If no more splits are possible\n",
        "  if splitting_var is None:\n",
        "    tree['split_by_feature'] = None\n",
        "    tree['error_of_split'] = None\n",
        "    tree['successor_left'] = None\n",
        "    tree['successor_right'] = None\n",
        "\n",
        "  else:\n",
        "    tree['split_by_feature'] = splitting_var\n",
        "    tree['error_of_split'] = results[results[feature_col] == splitting_var][totError].values[0]\n",
        "    left_df = df.query(f'{splitting_var} == 1')\n",
        "    left_df = left_df.drop(splitting_var, axis = 1)\n",
        "    right_df = df.query(f'{splitting_var} == 0')\n",
        "    right_df = right_df.drop(splitting_var, axis = 1)\n",
        "    \n",
        "    # Compute the performance of the left and right splits\n",
        "    results_left,results_right = compute_performance(left_df,right_df)\n",
        "    \n",
        "    # build the rest of the tree\n",
        "    tree['successor_left'] = build_tree(left_df, results_left, name=name+'L')\n",
        "    tree['successor_right'] = build_tree(right_df, results_right, name=name+'R')\n",
        "        \n",
        "  return tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZZwmNJ4CrFdM"
      },
      "outputs": [],
      "source": [
        "def get_cart(sample_set_csv):\n",
        "    # The sample_set_csv is a file path to a csv data, this can be imported into a dataframe\n",
        "    df = pd.read_csv(sample_set_csv)\n",
        "    \n",
        "    #build the cart tree\n",
        "    cart = build_tree(df)\n",
        "\n",
        "    return cart\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6hWqPbjrFdM"
      },
      "source": [
        "# Task 2a: Highest influencing feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hlVO8BqhrFdN"
      },
      "outputs": [],
      "source": [
        "def get_highest_influence_feature_helper(cart):\n",
        "    #initialize the outcomes\n",
        "    highest_feature = cart[\"name\"]\n",
        "    highest_error = cart[\"error_of_split\"]\n",
        "    feature = cart[\"split_by_feature\"]\n",
        "\n",
        "    # Check if the left successor exists\n",
        "    if cart[\"successor_left\"] != None:\n",
        "      left_feature, left_error, left_feature= get_highest_influence_feature_helper(cart[\"successor_left\"])\n",
        "      if (left_feature is not None and left_error is not None):\n",
        "        # Compare the left error with the current  error\n",
        "        if left_error > highest_error:\n",
        "          highest_feature = left_feature\n",
        "          highest_error = left_error\n",
        "          feature = left_feature\n",
        "\n",
        "    if cart[\"successor_right\"] != None:\n",
        "      right_feature, right_error, right_feature = get_highest_influence_feature_helper(cart[\"successor_right\"])\n",
        "      if (right_feature is not None and right_error is not None):\n",
        "        if right_error > highest_error:\n",
        "          highest_feature = right_feature\n",
        "          highest_error = right_error\n",
        "          feature = right_feature\n",
        "    #return name, error and original name of the feautre with highest influence\n",
        "    return highest_feature, highest_error , feature"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_highest_influence_feature(cart):\n",
        "   highest_feature, highest_error , feature_split = get_highest_influence_feature_helper(cart)\n",
        "   return feature_split"
      ],
      "metadata": {
        "id": "fXGU2mf25zIj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHTfhmDOrFdN"
      },
      "source": [
        "# Task 2b: Calculate the error rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eSMA6IazrFdN"
      },
      "outputs": [],
      "source": [
        "def predict_performance(tree, feature):\n",
        "  # if no more splits return the mean,\n",
        "    if tree['split_by_feature'] is None:\n",
        "        return tree['mean']\n",
        "    else:\n",
        "      # Check the value of the feature used for the split\n",
        "        feature_value = feature[tree['split_by_feature']]\n",
        "        if feature_value == 1:\n",
        "            return predict_performance(tree['successor_left'], feature)\n",
        "        else:\n",
        "            return predict_performance(tree['successor_right'], feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gxvJBDkorFdN"
      },
      "outputs": [],
      "source": [
        "def get_error_rate(cart, sample_set_csv):\n",
        "      # The sample_set_csv is a file path to a csv data, this can be imported into a dataframe\n",
        "      df = pd.read_csv(sample_set_csv)\n",
        "    \n",
        "      # Initialize error rate\n",
        "      error_rate = 0\n",
        "\n",
        "      for index, feature in df.iterrows():\n",
        "        #predict the performance per feature\n",
        "        pred_perf = predict_performance(cart, feature)\n",
        "        \n",
        "        #compute prediction error \n",
        "        error_rate = error_rate + abs(pred_perf - feature['performance'])\n",
        "\n",
        "      #make it average error rate\n",
        "      error_rate = error_rate/ len(df)\n",
        "\n",
        "      return error_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T76MNzFWrFdO"
      },
      "source": [
        "# Task 2c: Generate optimal configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_configuration(cart, partial_config):\n",
        "    optimal_config = set(partial_config)\n",
        "    #stop when split feature is none\n",
        "    if cart['split_by_feature'] is None:\n",
        "      #arrange optimal_config based on feature diagram \n",
        "        if \"zip\" in partial_config or cart:\n",
        "            if \"zip\" and \"rar\" not in optimal_config:\n",
        "                optimal_config.add(\"rar\")\n",
        "        elif \"rar\" in partial_config or cart:\n",
        "            if \"rar\" not in optimal_config:\n",
        "                optimal_config.add(\"zip\")\n",
        "        if \"encryption\" not in partial_config and cart:\n",
        "            for i in (\"aes\", \"blowfish\"):\n",
        "                if i in optimal_config:\n",
        "                  optimal_config.remove(i)\n",
        "        else:\n",
        "          if \"aes\" in partial_config or cart:\n",
        "            if \"aes\" not in optimal_config:\n",
        "              optimal_config.add(\"blowfish\")\n",
        "          if \"blowfish\" in partial_config or cart:\n",
        "            if \"blowfish\" not in optimal_config:\n",
        "              optimal_config.add(\"aes\")\n",
        "        if \"segmentation\" not in partial_config and cart:\n",
        "          for i in (\"onehundredmb\", \"onegb\"):\n",
        "            if i in optimal_config:\n",
        "              optimal_config.remove(i)\n",
        "        else:\n",
        "          if \"onehundredmb\" in partial_config or cart:\n",
        "            if \"onehundredmb\" not in optimal_config:\n",
        "              optimal_config.add(\"onegb\")\n",
        "          if \"onegb\" in partial_config or cart:\n",
        "            if \"onegb\" not in optimal_config:\n",
        "              optimal_config.add(\"onehundredmb\")\n",
        "        return optimal_config\n",
        "    else:\n",
        "        feature = cart['split_by_feature']\n",
        "        #seperate partial config to right and left\n",
        "        left_config = set(partial_config)\n",
        "        right_config = set(partial_config)\n",
        "        left_config.add(feature)\n",
        "\n",
        "        #predict performance for each \n",
        "        left_performance = predict_performance(cart['successor_left'], left_config)\n",
        "        right_performance = predict_performance(cart['successor_right'], right_config)\n",
        "\n",
        "        if left_performance <= right_performance:\n",
        "            return get_optimal_configuration(cart['successor_left'], left_config)\n",
        "        else:\n",
        "            return get_optimal_configuration(cart['successor_right'], right_config)\n"
      ],
      "metadata": {
        "id": "1AglRwJjeTRk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR2IOpt0rFdO"
      },
      "source": [
        "# Tests:  \n",
        "In the following cells, we provide you some test cases (but not all possible test cases!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clzGFCoNrFdO",
        "outputId": "41032016-272f-43c6-e516-5f7441bb23e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# Task 1\n",
        "\n",
        "test_cart = {'name': 'X', 'mean': 763.2, 'split_by_feature': 'segmentation', 'error_of_split': 6.0, \n",
        "             'successor_left': \n",
        "                 {'name': 'XL', 'mean': 772.0, 'split_by_feature': 'onegb', 'error_of_split': 0.0, \n",
        "                  'successor_left': \n",
        "                      {'name': 'XLL', 'mean': 770.0, 'split_by_feature': None, 'error_of_split': None, \n",
        "                       'successor_left': None, \n",
        "                       'successor_right': None\n",
        "                      }, \n",
        "                  'successor_right': \n",
        "                      {'name': 'XLR', 'mean': 773.0, 'split_by_feature': None, 'error_of_split': None, \n",
        "                       'successor_left': None, \n",
        "                       'successor_right': None\n",
        "                      }\n",
        "                 }, \n",
        "             'successor_right': \n",
        "                 {'name': 'XR', 'mean': 750.0, 'split_by_feature': None, 'error_of_split': None, \n",
        "                  'successor_left': None, \n",
        "                  'successor_right': None}\n",
        "            }\n",
        "\n",
        "\n",
        "if get_cart(\"Performance_01.csv\") == test_cart:\n",
        "    print(\"passed\")\n",
        "else:\n",
        "    print(\"failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDWsC17CrFdP",
        "outputId": "a8d3f9bd-542d-407c-c5b1-f27f194cda66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# Task 2b\n",
        "if get_error_rate(test_cart, \"Performance_02b.csv\") == 5:\n",
        "    print(\"passed\")\n",
        "else:\n",
        "    print(\"failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hRJ0Fp6xrFdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b682a1-33c0-4ea0-cc26-48cbaaed7ed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passed\n"
          ]
        }
      ],
      "source": [
        "# Task 2c\n",
        "test_cart_v2 = {'name': 'X', 'mean': 763.2, 'split_by_feature': 'zip', 'error_of_split': 0.0, \n",
        "                 'successor_left': {'name': 'XL', 'mean': 772.0, 'split_by_feature': None, 'error_of_split': None, \n",
        "                                    'successor_left': None, \n",
        "                                    'successor_right': None}, \n",
        "                 'successor_right': {'name': 'XR', 'mean': 750.0, 'split_by_feature': None, 'error_of_split': None, \n",
        "                                     'successor_left': None, \n",
        "                                     'successor_right': None}\n",
        "                }\n",
        "\n",
        "optimal_config = get_optimal_configuration(test_cart_v2, {\"secompress\", \"encryption\", \"aes\", \"algorithm\", \"signature\",\n",
        "                                                        \"timestamp\", \"segmentation\", \"onehundredmb\"})\n",
        "reference = {'aes', 'algorithm', 'encryption', 'onehundredmb', 'rar', 'secompress', 'segmentation', 'signature',\n",
        "            'timestamp'}\n",
        "\n",
        "if optimal_config == reference:\n",
        "    print(\"passed\")\n",
        "else:\n",
        "    print(\"failed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}